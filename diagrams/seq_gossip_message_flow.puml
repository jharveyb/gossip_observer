@startuml seq_gossip_message_flow
skinparam defaultFontSize 11
skinparam shadowing false

title Gossip Message -- End-to-End Flow

participant "LN Peer" as peer
participant "LDK Node" as ldk
participant "NATSExporter\n(LogWriter)" as exporter
participant "queue_exported_msg\ntask" as queue
participant "publish_msgs\ntask" as pub
participant "NATS\nJetStream" as nats
participant "nats_reader\ntask" as reader
participant "msg_decoder\ntask" as decode
participant "db_write_handler\ntask" as dbw
database "TimescaleDB" as db

note over peer, ldk : Lightning P2P protocol
note over ldk, pub : gossip_collector process
note over reader, dbw : gossip_archiver process

== Gossip Arrives ==

peer -> ldk : node_announcement /\nchannel_announcement /\nchannel_update
activate ldk

ldk -> ldk : GossipMessageHandler\nprocesses & validates

ldk -> exporter : LogWriter.export()\n(sync callback)
activate exporter

exporter -> exporter : Format CSV line:\n"ts,peer,type,dir,size,...,msg,collector_id"
exporter -> queue : export_tx.send(csv_line)\n(unbounded channel)
deactivate exporter
deactivate ldk

== Batching & Filtering ==

activate queue
queue -> queue : Check PeerConnManager:\nis peer in eligible set?

alt peer is eligible
    queue -> queue : Accumulate message\nin batch buffer

    alt batch full (N=1024) OR timeout
        queue -> pub : nats_tx.send(batch)\n(bounded channel)
        activate pub
    end
else peer not eligible
    queue -> queue : Drop message\n(not monitoring this peer)
end
deactivate queue

== Publish to NATS ==

pub -> pub : Join batch with ";" delimiter
pub -> pub : Compute message_id =\nXXH3_64(payload)
pub -> nats : publish("observer.{collector_pk}",\npayload, message_id)
activate nats
nats --> pub : ACK (JetStream)
deactivate pub

== Archiver Consumes ==

nats -> reader : consumer.messages()\n(durable pull consumer)
deactivate nats
activate reader

reader -> reader : Split on ";" into\nindividual messages
reader -> decode : raw_msg_tx.send(msg)\n(unbounded channel)
activate decode
reader -> nats : msg.ack()

deactivate reader

== Decode ==

decode -> decode : Parse CSV fields →\nExportedGossip struct
decode -> decode : Extract: type, peer_hash,\ninner_hash, scid, orig_node
decode -> dbw : msg_tx.send(ExportedGossip)\n(unbounded channel)
deactivate decode

== Batched DB Write ==

activate dbw
dbw -> dbw : Buffer messages until:\nbatch_size (10000) OR\ntimeout (5s)

dbw -> db : BEGIN transaction
activate db
dbw -> db : COPY INTO **messages**\n(hash → raw text)
dbw -> db : COPY INTO **timings**\n(net_ts, peer, collector, dir)
dbw -> db : COPY INTO **metadata**\n(hash → type, size, scid, node)
dbw -> db : COPY INTO **message_hashes**\n(outer_hash → inner_hash)
dbw -> db : COMMIT
db --> dbw : OK
deactivate db
deactivate dbw

== Continuous Aggregate Refresh ==

note over db : Periodic materialization\n(cron / TimescaleDB policy)
db -> db : Refresh ca_peer_count_10m
db -> db : Refresh ca_message_rate_10m
db -> db : Refresh ca_msg_propagation_global_1h
db -> db : Refresh ca_orig_node_activity_1h\n...

@enduml
